{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5MUmPXWY7YY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain>=0.1.17 openai>=1.13.3 langchain_openai>=0.1.6 transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 sentence-transformers>=2.5.1 duckduckgo-search>=5.2.2 langchain_community\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python==0.2.69"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain, LlamaCpp"
      ],
      "metadata": {
        "id": "s1wMBu66ikMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the model path is correct for your system\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=2048,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "llm.invoke(\"Hi! My name is Maarten. What is 2 + 1?\")"
      ],
      "metadata": {
        "id": "-l_LmVFCcW6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the prompt_template with input_prompt variable\n",
        "template = \"\"\"<s><|user|>\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\"]\n",
        ")\n",
        "\n",
        "# Basic chain : Prompt + LLM\n",
        "basic_chain = prompt | llm\n",
        "\n",
        "# Use the chain\n",
        "basic_chain.invoke(\n",
        "    {\n",
        "        \"input_prompt\": \"Hi! My name is Maarten. What is 2 + 1?\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "_k2nJNj8hhn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A chain with multiple prompts. Create a chain for the title of our story.\n",
        "template = \"\"\"<s><|user|>\n",
        "Create a title for a tory about {summary}. Only return the title.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
        "title = LLMChain(llm=llm, prompt=title_prompt, output_key='title')\n",
        "\n",
        "title.invoke({\"summary\": \"A girl that lost her mother.\"})"
      ],
      "metadata": {
        "id": "LeZ1gSLEtv2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chain for the character description using the summary and the title\n",
        "template = \"\"\"<s><|user|>\n",
        "Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "character_prompt = PromptTemplate(template=template, input_variables=[\"summary\", \"title\"])\n",
        "character = LLMChain(llm=llm, prompt=character_prompt, output_key='character')\n",
        "\n",
        "#character.invoke({\"summary\": \"A girl that lost her mother.\", \"title\": \"The Lost Mother\"})\n"
      ],
      "metadata": {
        "id": "qBWFgOcpAT1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chain for the story using the summary, title and character description\n",
        "template = \"\"\"<s><|user|>\n",
        "Create a story about {summary} with the title {title}. The main character is: {character}. Only return the story and it cannot be longer than one paragraph.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "story_prompt = PromptTemplate(template=template, input_variables=[\"summary\", \"title\", \"character\"])\n",
        "story = LLMChain(llm=llm, prompt=story_prompt, output_key='story')"
      ],
      "metadata": {
        "id": "A4LEZA1wKoHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all three components to create the full chain\n",
        "llm_chain = title | character | story"
      ],
      "metadata": {
        "id": "fPq-ovgALeVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke(\"A girl that lost her mother.\")"
      ],
      "metadata": {
        "id": "BTToNbR3Lq5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Memory: Helping LLMs to Remember Conversations"
      ],
      "metadata": {
        "id": "sKQ1FpKAlhPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationSummaryMemory"
      ],
      "metadata": {
        "id": "hjdPH-4wmlol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversation Buffere Memory: Append the chat history to the prompt\n",
        "\n",
        "# Create an updated prompt template to include chat history\n",
        "template = \"\"\"<s><|user|>Current conversation: {chat_history}\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GT7v11C1lo59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the type of memory we will use\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# Chain the LLM, prompt and memory together\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "A4iwYKwlmqUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a conversation and ask a basic question\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 2 + 1?\"})\n",
        "\n",
        "# Does the LLM remember the name we gave it?\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ],
      "metadata": {
        "id": "4KznRhlnnRV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Windowed Conversation Buffer"
      ],
      "metadata": {
        "id": "OqkVh9BoqdBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retain only the last two conversations in the memory\n",
        "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
        "\n",
        "# Chain the LLM, prompt and memory together\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "tNmGb09wqhnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask two questions and generate two conversations in its memory\n",
        "\n",
        "llm_chain.predict(input_prompt=\"Hi! My name is Maarten and I am 33 years old. What is 2 + 1?\")\n",
        "llm_chain.predict(input_prompt=\"What is 3 + 3?\")"
      ],
      "metadata": {
        "id": "nH0_FuoYsKHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether it knows the name we gave it\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ],
      "metadata": {
        "id": "G3KjldyHt9mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether it knows the age we gave it (Note that llm_chain is supposed to remember only last two conversations)\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my age?\"})"
      ],
      "metadata": {
        "id": "lUI8gyAbvOCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Summary"
      ],
      "metadata": {
        "id": "jN38QIzXsGSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a summary prompt template\n",
        "summary_prompt_template = \"\"\"<s><|user|>Summarize the conversations and update with the new lines.\n",
        "Current summary: {summary}\n",
        "\n",
        "New lines of conversation: {new_lines}\n",
        "\n",
        "New summary:\n",
        "<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "summary_prompt = PromptTemplate(\n",
        "    template=summary_prompt_template,\n",
        "    input_variables=[\"summary\", \"new_lines\"])"
      ],
      "metadata": {
        "id": "y2-xWoamzmKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the type of memory we will use\n",
        "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", prompt=summary_prompt)\n",
        "\n",
        "# Chain the LLM, prompt and memory together\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "ysGZIeqaESu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a conversation and ask for the name\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 2 + 1?\"})\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ],
      "metadata": {
        "id": "2UakMFHuFXTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether everything has been summarized thus far\n",
        "llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"
      ],
      "metadata": {
        "id": "PcwZm-qBGZRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing the memory variable that we created to check what the summary is thus far\n",
        "memory.load_meamory_variables({})"
      ],
      "metadata": {
        "id": "ntbinvwRHaQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents: Creating a System of LLMs"
      ],
      "metadata": {
        "id": "S9VCTSccIdKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReAct in LangChain with OpenAI's GPT-3.5"
      ],
      "metadata": {
        "id": "33rQasQ0U91P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import load_tools, Tool\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain.tools import DuckDuckGoSearchRun"
      ],
      "metadata": {
        "id": "8ErcoiO2Ig2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load OpenAI's LLMs with LangChain\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "Ofmu8pYFVUcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the ReAct template\n",
        "react_template = \"\"\"Answer the following questions as best as you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: The action to take should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought: {agent_scratchpad}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=react_template,\n",
        "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
        ")"
      ],
      "metadata": {
        "id": "x_FOyL5wWAaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can create the tool to pass to an agent\n",
        "search = DuckDuckGoSearchResults()\n",
        "search_tool = Tool(\n",
        "    name=\"duckduck\",\n",
        "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
        "    func=search.run\n",
        ")\n",
        "\n",
        "# Prepare tools\n",
        "tools = load_tools([\"llm_math\"], llm=openai_llm)\n",
        "tools.append(search_tool)"
      ],
      "metadata": {
        "id": "MxnSz4F5fGcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the ReAct agent\n",
        "\n",
        "agent = create_react_agent(openai_llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent, tools=tools, verbose=True, handle_parser_errors=True\n",
        ")"
      ],
      "metadata": {
        "id": "zNeWN3lImQPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the price of a MacBook Pro?\n",
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "FoS4hVWbm8fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9OVvfasuV_Lr"
      }
    }
  ]
}